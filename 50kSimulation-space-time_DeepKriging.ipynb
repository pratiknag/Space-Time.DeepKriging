{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feeebfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.python.keras.models import Sequential,Model\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, BatchNormalization,Input, LSTM, Add, Subtract, Lambda\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras import regularizers,initializers\n",
    "import keras.backend as Kr\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "# Library for Gaussian process\n",
    "# import GPy\n",
    "##Library for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib;matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "import pylab \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# sess = tf.compat.v1.Session()\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.python.keras import backend as K\n",
    "\n",
    "# # adjust values to your needs\n",
    "# config = tf.compat.v1.ConfigProto( device_count = {'GPU': 2 , 'CPU': 30} )\n",
    "# sess = tf.compat.v1.Session(config=config) \n",
    "# K.set_session(sess)\n",
    "\n",
    "HeUniform = tf.keras.initializers.he_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"synthetic_50000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ee78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x\"] = df[\"V1\"]\n",
    "df[\"y\"] = df[\"V2\"]\n",
    "df[\"t\"] = df[\"V3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test locations \n",
    "a = np.linspace(0,1,100)\n",
    "\n",
    "s1, s2 = np.meshgrid(a,a)\n",
    "s_test = np.column_stack((s1.flatten(),s2.flatten()))\n",
    "s_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aac03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "s = np.array(df[\"t\"]/500).reshape(len(df),1)\n",
    "print(s.shape)\n",
    "N_data = len(df)\n",
    "\n",
    "t = np.array([250,350,450])/500\n",
    "s_t = np.repeat(t, 10000).reshape(3*10000,1)\n",
    "print(s_t.shape)\n",
    "\n",
    "s = np.vstack((s,s_t))\n",
    "N = N_data + len(s_t)\n",
    "print(N)\n",
    "\n",
    "## time basis \n",
    "num_basis = [70,250,410]\n",
    "std_arr = [0.2,0.09,0.009]\n",
    "#std_arr = [0.3,0.15,0.05]\n",
    "mu_knots = [np.linspace(0,1,int(i)) for i in num_basis]\n",
    "\n",
    "phi_t = np.zeros((N, sum(num_basis)))\n",
    "K = 0\n",
    "for res in range(len(num_basis)):\n",
    "    std = std_arr[res]\n",
    "    for i in range(num_basis[res]):\n",
    "        d = np.square(np.absolute(s-mu_knots[res][i]))\n",
    "        for j in range(len(d)):\n",
    "            if d[j] >= 0 and d[j] <= 1:\n",
    "                phi_t[j,i + K] = np.exp(-0.5 * d[j]/(std**2))\n",
    "            else:\n",
    "                phi_t[j,i + K] = 0\n",
    "    K = K + num_basis[res]\n",
    "\n",
    "\n",
    "# # time basis\n",
    "# num_basis = [3,7,11]\n",
    "# knots = [np.linspace(0,1,i) for i in num_basis]\n",
    "# ##Wendland kernel\n",
    "# K = 0 ## basis size\n",
    "# phi_t = np.zeros((N, sum(num_basis)))\n",
    "# for res in range(len(num_basis)):\n",
    "#     theta = 1/num_basis[res]*2.5\n",
    "#     for i in range(num_basis[res]):\n",
    "#         d = np.absolute(s-knots[res][i])/theta\n",
    "#         for j in range(len(d)):\n",
    "#             if d[j] >= 0 and d[j] <= 1:\n",
    "#                 phi_t[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "#             else:\n",
    "#                 phi_t[j,i + K] = 0\n",
    "#     K = K + num_basis[res]\n",
    "\n",
    "s = np.vstack((df[\"x\"],df[\"y\"])).T\n",
    "\n",
    "s = np.vstack((s,s_test,s_test,s_test))\n",
    "\n",
    "# space basis\n",
    "num_basis = [5**2,9**2,11**2]\n",
    "knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
    "##Wendland kernel\n",
    "K = 0\n",
    "phi = np.zeros((N, sum(num_basis)))\n",
    "for res in range(len(num_basis)):\n",
    "    theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "    knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
    "    knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
    "    for i in range(num_basis[res]):\n",
    "        d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
    "        for j in range(len(d)):\n",
    "            if d[j] >= 0 and d[j] <= 1:\n",
    "                phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "            else:\n",
    "                phi[j,i + K] = 0\n",
    "    K = K + num_basis[res]\n",
    "\n",
    "# phi2 = np.zeros((phi_t.shape[0],(phi_t.shape[1]*phi.shape[1])))\n",
    "\n",
    "# for i in range(phi_t.shape[0]):\n",
    "#     full_prod = []\n",
    "#     for j in range(len(phi_t[i])):\n",
    "#         if j == 0 : full_prod = phi[i]*phi_t[i,j]\n",
    "#         else:\n",
    "#             product = phi[i]*phi_t[i,j]\n",
    "#             full_prod = np.concatenate((full_prod, product), axis=None)\n",
    "#     phi2[i] = full_prod\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phi_t.shape)\n",
    "print(phi.shape)\n",
    "\n",
    "phi2 = np.hstack((phi_t,phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af24f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Romove the all-zero columns\n",
    "idx_zero = np.array([], dtype=int)\n",
    "for i in range(phi2.shape[1]):\n",
    "    if sum(phi2[:,i]!=0)==0:\n",
    "        idx_zero = np.append(idx_zero,int(i))\n",
    "\n",
    "phi_reduce = np.delete(phi2,idx_zero,1)\n",
    "print(phi2.shape)\n",
    "print(phi_reduce.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff96466",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embedding_50k.npy\", phi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607926cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df[\"nonstat_z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fa324",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_reduce = np.load(\"embedding_50k.npy\")\n",
    "print(phi_reduce.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_data = len(df)\n",
    "phi_reduce_train = phi_reduce[0:N_data,]\n",
    "phi_reduce_test = phi_reduce[N_data:phi_reduce.shape[0],]\n",
    "print(phi_reduce_test.shape)\n",
    "print(phi_reduce_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47fdcb",
   "metadata": {},
   "source": [
    "## Deep Learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for calculation of MSE and MAE\n",
    "def mse(y_pred,y_true):\n",
    "    mse = np.mean((y_pred-y_true)**2)\n",
    "    return mse\n",
    "\n",
    "def mae(y_pred,y_true):\n",
    "    mae = np.mean(np.absolute(y_pred-y_true))\n",
    "    return mae\n",
    "\n",
    "mse_var = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d5ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Modeling the median\n",
    "start_time = time.time()\n",
    "x_train,x_test,y_train, y_test= train_test_split(phi_reduce_train, y, \n",
    "                                                        test_size=0.1)\n",
    "\n",
    "q = 0.5\n",
    "def tilted_loss(y,f):\n",
    "\n",
    "    e1 = (y-f)\n",
    "    the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
    "    return the_sum\n",
    "#     data_train = np.hstack((encoder_train,y_train))\n",
    "#     n_rows = data_train.shape[0]\n",
    "#     random_indices = np.random.choice(n_rows, size=10000, replace=True)\n",
    "#     resampled_data_train = data_train[random_indices, :]\n",
    "# DeepKriging model for continuous data\n",
    "model = Sequential()\n",
    "# model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "model.add(Dense(100, input_dim = x_train.shape[1],  \n",
    "                kernel_initializer='he_uniform', activation='relu'))\n",
    "# model.add(Dropout(rate=0.5))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5),activation='relu'))\n",
    "model.add(Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5),activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "#model.add(Dropout(rate=0.5))\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='linear'))\n",
    "NB_START_EPOCHS = 50 \n",
    "# NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss=tilted_loss)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience= 30),\n",
    "         ModelCheckpoint(filepath='model_real-50k.h5', monitor='val_loss', save_best_only=True)]\n",
    "result = model.fit(x_train, y_train,callbacks = callbacks, \n",
    "                   validation_data=(x_test,y_test), epochs = 350, batch_size = 512, verbose = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     result = model.fit(x_train, y_train, callbacks=callbacks, \n",
    "#                        validation_data=(x_test,y_test), epochs = 200, batch_size = 64, verbose = 2)\n",
    "model = keras.models.load_model('model_real-50k.h5',custom_objects={'tilted_loss':tilted_loss})\n",
    "# y_pred = model.predict(x_test)\n",
    "\n",
    "# Mean Squared Error\n",
    "# mse_var.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"%s seconds\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b3d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Modeling the quantile functions \n",
    "start_time = time.time()\n",
    "model1 = keras.models.load_model('model_real-50k.h5')\n",
    "medY = model1.predict(phi_reduce_train)\n",
    "x_train,x_test,y_train, y_test, medY_train, medY_test= train_test_split(phi_reduce_train, y, medY,\n",
    "                                                        test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "q = 0.95\n",
    "def tilted_loss(y,f):\n",
    "\n",
    "    e1 = (y-f)\n",
    "    the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
    "    return the_sum\n",
    "#     data_train = np.hstack((encoder_train,y_train))\n",
    "#     n_rows = data_train.shape[0]\n",
    "#     random_indices = np.random.choice(n_rows, size=10000, replace=True)\n",
    "#     resampled_data_train = data_train[random_indices, :]\n",
    "# DeepKriging model for continuous data\n",
    "imp = Input(shape=x_train.shape[1])\n",
    "input_medY = Input(shape = 1)\n",
    "# model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "x = Dense(100,kernel_initializer='he_uniform', activation='relu')(imp)\n",
    "# model.add(Dropout(rate=0.5))\n",
    "# model.add(BatchNormalization())\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5),activation='relu')(x)\n",
    "x = Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5,l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5),activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "#model.add(Dropout(rate=0.5))\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "x = Lambda(lambda d: d * 10)(x)\n",
    "x = Add()([input_medY,x]) #### comment out for quantile 95\n",
    "# x = Subtract()([input_medY,x]) #### comment out for quantile 05\n",
    "model = Model(inputs=[imp, input_medY], outputs=x)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss=tilted_loss)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience= 30),\n",
    "         ModelCheckpoint(filepath='model_real-50k_95.h5', monitor='val_loss', save_best_only=True)]\n",
    "result = model.fit([x_train,medY_train], y_train,callbacks = callbacks, \n",
    "                   validation_data=([x_test,medY_test],y_test), epochs = 350, batch_size = 512, verbose = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     result = model.fit(x_train, y_train, callbacks=callbacks, \n",
    "#                        validation_data=(x_test,y_test), epochs = 200, batch_size = 64, verbose = 2)\n",
    "# model = keras.models.load_model('model_real-50k_05.h5',custom_objects={'tilted_loss':tilted_loss})\n",
    "# y_pred = model.predict(x_test)\n",
    "\n",
    "# Mean Squared Error\n",
    "# mse_var.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"%s seconds\", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da607fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0.95\n",
    "def tilted_loss(y,f):\n",
    "\n",
    "    e1 = (y-f)\n",
    "    the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
    "    return the_sum\n",
    "model1 = keras.models.load_model('model_real-50k_05.h5',custom_objects={'tilted_loss':tilted_loss})\n",
    "model2 = keras.models.load_model('model_real-50k_95.h5',custom_objects={'tilted_loss':tilted_loss})\n",
    "model_med = keras.models.load_model('model_real-50k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45519823",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Locations for interpolation\n",
    "\n",
    "\n",
    "\n",
    "# t = (np.array(range(286)) + 1)/286\n",
    "s = np.array(np.unique(df[\"t\"]))/500\n",
    "\n",
    "# print(s_t.shape)\n",
    "\n",
    "# s = np.vstack((s,s_t))\n",
    "N = 500\n",
    "# print(N)\n",
    "\n",
    "## time basis \n",
    "num_basis = [70,250,410]\n",
    "std_arr = [0.2,0.09,0.009]\n",
    "#std_arr = [0.3,0.15,0.05]\n",
    "mu_knots = [np.linspace(0,1,int(i)) for i in num_basis]\n",
    "\n",
    "phi_t = np.zeros((N, sum(num_basis)))\n",
    "K = 0\n",
    "for res in range(len(num_basis)):\n",
    "    std = std_arr[res]\n",
    "    for i in range(num_basis[res]):\n",
    "        d = np.square(np.absolute(s-mu_knots[res][i]))\n",
    "        for j in range(len(d)):\n",
    "            if d[j] >= 0 and d[j] <= 1:\n",
    "                phi_t[j,i + K] = np.exp(-0.5 * d[j]/(std**2))\n",
    "            else:\n",
    "                phi_t[j,i + K] = 0\n",
    "    K = K + num_basis[res]\n",
    "\n",
    "\n",
    "# # time basis\n",
    "# num_basis = [3,7,11]\n",
    "# knots = [np.linspace(0,1,i) for i in num_basis]\n",
    "# ##Wendland kernel\n",
    "# K = 0 ## basis size\n",
    "# phi_t = np.zeros((N, sum(num_basis)))\n",
    "# for res in range(len(num_basis)):\n",
    "#     theta = 1/num_basis[res]*2.5\n",
    "#     for i in range(num_basis[res]):\n",
    "#         d = np.absolute(s-knots[res][i])/theta\n",
    "#         for j in range(len(d)):\n",
    "#             if d[j] >= 0 and d[j] <= 1:\n",
    "#                 phi_t[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "#             else:\n",
    "#                 phi_t[j,i + K] = 0\n",
    "#     K = K + num_basis[res]\n",
    "\n",
    "s = np.repeat([[0.077215,0.041551]],500).reshape(500,2)\n",
    "\n",
    "# space basis\n",
    "num_basis = [5**2,9**2,11**2]\n",
    "knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
    "##Wendland kernel\n",
    "K = 0\n",
    "phi = np.zeros((N, sum(num_basis)))\n",
    "for res in range(len(num_basis)):\n",
    "    theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "    knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
    "    knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
    "    for i in range(num_basis[res]):\n",
    "        d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
    "        for j in range(len(d)):\n",
    "            if d[j] >= 0 and d[j] <= 1:\n",
    "                phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "            else:\n",
    "                phi[j,i + K] = 0\n",
    "    K = K + num_basis[res]\n",
    "            \n",
    "        \n",
    "phi_test = np.hstack((phi_t,phi))        \n",
    "phi_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(phi_reduce_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef76a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d765e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For single location prediction\n",
    "\n",
    "df_pred1LOC = pd.DataFrame({\"pred\":predictions[:,0]})\n",
    "df_pred1LOC.to_csv(\"1Loc_interpolation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1dd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({\"x\":s_test[:,0],\"y\":s_test[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[\"predictions_t250\"] = predictions[0:10000,]\n",
    "df_pred[\"predictions_t350\"] = predictions[10000:20000,]\n",
    "df_pred[\"predictions_t450\"] = predictions[20000:30000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da61b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_med.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This section is only for storing the prediction intervals\n",
    "df_pred = pd.read_csv(\"50k_interpolation.csv\")\n",
    "pred_med = model_med.predict(phi_reduce_test)\n",
    "predictions1 = model1.predict([phi_reduce_test,pred_med])\n",
    "predictions2 = model2.predict([phi_reduce_test,pred_med])\n",
    "\n",
    "df_pred[\"lb_t250\"] = predictions1[0:10000,]\n",
    "df_pred[\"lb_t350\"] = predictions1[10000:20000,]\n",
    "df_pred[\"lb_t450\"] = predictions1[20000:30000,]\n",
    "\n",
    "df_pred[\"ub_t250\"] = predictions2[0:10000,]\n",
    "df_pred[\"ub_t350\"] = predictions2[10000:20000,]\n",
    "df_pred[\"ub_t450\"] = predictions2[20000:30000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bed11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df_pred[\"ub_t250\"] - df_pred[\"lb_t250\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv(\"50k_interpolation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d025960",
   "metadata": {},
   "source": [
    "## Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c6f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"50k_lstm_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array(df.iloc[:,5])\n",
    "\n",
    "split_size = 495\n",
    "train = Z[:split_size].reshape(split_size,1)\n",
    "# test = Z[split:].reshape(65,1)\n",
    "\n",
    "n_steps = 5\n",
    "n_output = 1\n",
    "training_size = split_size - (n_steps+n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((training_size,n_steps,1))\n",
    "y = np.zeros((training_size,n_output,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    # find the end of this pattern\n",
    "    end_ix = i + n_steps\n",
    "    # check if we are beyond the sequence\n",
    "    if end_ix > len(train)-1:\n",
    "        break\n",
    "    # gather input and output parts of the pattern\n",
    "    seq_x, seq_y = train[i:end_ix], train[end_ix:(end_ix+n_output)]\n",
    "    x[i] = seq_x\n",
    "    y[i] = seq_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = np.arange(x.shape[0])\n",
    "# train_index = indexes[: int(0.95 * x.shape[0])]\n",
    "# val_index = indexes[int(0.95 * x.shape[0]) :]\n",
    "\n",
    "# x_train, y_train = x[train_index],y[train_index]\n",
    "# x_val, y_val = x[val_index],y[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_function(q,x,y):\n",
    "    def tilted_loss(y,f):\n",
    "\n",
    "        e1 = (y-f)\n",
    "        the_sum = (Kr.mean(Kr.maximum(q*e1, (q-1)*e1), axis=-1))\n",
    "        return the_sum\n",
    "\n",
    "    # define model\n",
    "#     n_steps = 25\n",
    "    n_features = 1\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    # model.add(Dropout(0.1))\n",
    "    # model.add(LSTM(50, activation='relu',return_sequences=True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(LSTM(50, activation='relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(Dense(50, input_dim = x_train.shape[1],  \n",
    "    #                 kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dense(50, activation='relu'))\n",
    "    # # model.add(Dense(50, activation='relu'))\n",
    "    # model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(n_output,activation='linear'))\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss=tilted_loss, optimizer=optimizer)\n",
    "    print(\"running model for quantile level \"+str(q)+\".\")\n",
    "    result1 = model.fit(x, y, #callbacks = callbacks, \n",
    "                    batch_size = 128, \n",
    "          validation_split = 0.05, epochs=120, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b783c253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=330),\n",
    "#          ModelCheckpoint(filepath='lstm_2a_3.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "model1 = model_function(0.05,x,y)\n",
    "model2 = model_function(0.5,x,y)\n",
    "model3 = model_function(0.95,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test = Z[(split_size-n_steps):split_size].reshape(1,n_steps,1)\n",
    "pred05 = np.empty((0,1))\n",
    "pred5 = np.empty((0,1))\n",
    "pred95 = np.empty((0,1))\n",
    "\n",
    "for i in range(5):\n",
    "    pred1 = model1.predict(Z_test)\n",
    "    pred2 = model2.predict(Z_test)\n",
    "    pred3 = model3.predict(Z_test)\n",
    "    \n",
    "    Z_test = np.append(Z_test[:,:(n_steps-1),:],pred2).reshape(1,n_steps,1)\n",
    "    pred05 = np.append(pred05,pred1)\n",
    "    pred5 = np.append(pred5,pred2)\n",
    "    pred95 = np.append(pred95,pred3)\n",
    "\n",
    "mean_pred = np.empty((0,1))\n",
    "\n",
    "for i in range(len(x)):\n",
    "    if i == (training_size - 1):\n",
    "        pred2 = model2.predict(x[i,:,:].reshape(1,n_steps,1))\n",
    "        mean_pred = np.append(mean_pred,pred2)\n",
    "    else:\n",
    "        pred2 = model2.predict(x[i,:,:].reshape(1,n_steps,1))\n",
    "        mean_pred = np.append(mean_pred,pred2[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Z[split_size:].reshape(5,1)\n",
    "mse = mean_squared_error(pred5, test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(500), Z,label = \"true\")\n",
    "\n",
    "# plt.scatter(range(959), x_train[:,0])\n",
    "# plt.scatter(range(959), x_train[:,1])\n",
    "plt.plot(range(6,495), mean_pred, label = \"model mean\")\n",
    "plt.plot(range(495,500), pred05, label = \"model 0.05\")\n",
    "plt.plot(range(495,500), pred5, label = \"model 0.5\")\n",
    "plt.plot(range(495,500), pred95, label = \"model 0.95\")\n",
    "# plt.plot(range(3,100), pred_list95, label = \"model 0.95\")\n",
    "\n",
    "# plt.plot(range(97,100), pred_list[0,:], label = \"model 0.05\")\n",
    "# plt.plot(range(97,100), pred_list[1,:], label = \"model 0.5\")\n",
    "# plt.plot(range(97,100), pred_list[2,:], label = \"model 0.95\")\n",
    "\n",
    "plt.axvline(x=495)\n",
    "# plt.plot(range(959), pred_list[:,1])\n",
    "# plt.plot(range(959), pred_list[:,2])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471ff6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Run the model for all 50 time points #########\n",
    "mse = np.empty((0,1))\n",
    "mpiw = np.empty((0,1))\n",
    "for ind in range(50):\n",
    "    \n",
    "    Z = np.array(df.iloc[:,ind])\n",
    "\n",
    "    split_size = 495\n",
    "    train = Z[:split_size].reshape(split_size,1)\n",
    "    # test = Z[split:].reshape(65,1)\n",
    "\n",
    "    n_steps = 5\n",
    "    n_output = 1\n",
    "    training_size = split_size - (n_steps+n_output)\n",
    "    x = np.zeros((training_size,n_steps,1))\n",
    "    y = np.zeros((training_size,n_output,1))\n",
    "    for i in range(len(x)):\n",
    "    # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(train)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = train[i:end_ix], train[end_ix:(end_ix+n_output)]\n",
    "        x[i] = seq_x\n",
    "        y[i] = seq_y\n",
    "    \n",
    "    model1 = model_function(0.05,x,y)\n",
    "    model2 = model_function(0.5,x,y)\n",
    "    model3 = model_function(0.95,x,y)\n",
    "    Z_test = Z[(split_size-n_steps):split_size].reshape(1,n_steps,1)\n",
    "    pred05 = np.empty((0,1))\n",
    "    pred5 = np.empty((0,1))\n",
    "    pred95 = np.empty((0,1))\n",
    "\n",
    "    for i in range(5):\n",
    "        pred1 = model1.predict(Z_test)\n",
    "        pred2 = model2.predict(Z_test)\n",
    "        pred3 = model3.predict(Z_test)\n",
    "\n",
    "        Z_test = np.append(Z_test[:,:(n_steps-1),:],pred2).reshape(1,n_steps,1)\n",
    "        pred05 = np.append(pred05,pred1)\n",
    "        pred5 = np.append(pred5,pred2)\n",
    "        pred95 = np.append(pred95,pred3)\n",
    "\n",
    "#     for i in range(len(x)):\n",
    "#         if i == 429:\n",
    "#             pred2 = model2.predict(x[i,:,:].reshape(1,15,1))\n",
    "#             mean_pred = np.append(mean_pred,pred2)\n",
    "#         else:\n",
    "#             pred2 = model2.predict(x[i,:,:].reshape(1,15,1))\n",
    "#             mean_pred = np.append(mean_pred,pred2[:,0])\n",
    "    mpiw = np.append(mpiw,np.mean(pred95 - pred05))\n",
    "    test = Z[split_size:].reshape(n_steps,1)\n",
    "    mse = np.append(mse,mean_squared_error(pred5, test))\n",
    "    print(ind)\n",
    "    print(mse)\n",
    "    print(mpiw)\n",
    "    print(\"################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred05 = np.append(mean_pred,pred05)\n",
    "pred95 = np.append(mean_pred,pred95)\n",
    "\n",
    "pred5 = np.append(mean_pred,pred5)\n",
    "\n",
    "z = Z[6:]\n",
    "df_pred = pd.DataFrame()\n",
    "\n",
    "df_pred[\"z\"] = z\n",
    "df_pred[\"interval_05\"] = pred05\n",
    "df_pred[\"interval_5\"] = pred5\n",
    "df_pred[\"interval_95\"] = pred95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv(\"simulation_time_series_loc2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
